<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <link rel="stylesheet" href="../cme216_slides.css">
</head>

<body>
    <textarea id="source">
class: center, middle

# CME 216, ME 343 - Spring 2020

## Eric Darve, ICME

![:width 40%](../Stanford.jpg)

---
class: center, middle

# What is TensorFlow?

![:width 20vw](tf.svg)

---
class: middle

TF can do many things. 

It's free and open source.

Obviously it can be used for deep learning.

---
class: middle

But at its base, it allows to express computations as a graph.

Then this graph can be differentiated, e.g., we can calculate gradients and solve optimization problems using gradient descent methods.

---
class: middle

For now, we will skip these general features of TF and focus on DNN but we will come back to that when discussing physics-informed machine learning or PhysML.

---
class: middle

TensorFlow was developed by the [Google Brain](https://research.google/teams/brain/) team for internal use at Google. 

Then it was made open source in November 2015.

---
class: center, middle

# What is Keras then?

![:width 20vw](keras.png)

---
class: middle

Keras is an API (application programming interface) for DNN calculations.

API means that Keras defines a specific interface to write computer programs.

Keras is written in Python.

---
class: middle

Keras has multiple backends (that is libraries that are responsible for doing the actual calculations).

---
class: middle

At this time they include:

- TensorFlow
- [CNTK](https://github.com/Microsoft/cntk) (from Microsoft)
- [Theano](https://github.com/Theano/Theano) (University of Montr&eacute;al, Canada)

---
class: middle

Compared to TF, Keras is focused on easy and fast prototyping, through

- user friendliness, 
- modularity, and 
- extensibility

---
class: middle

Although TF can be used as a backend for Keras, it is recommended to use `tf.keras`, which is the implementation of Keras in TF.

This is what we will do in this class.

---
class: center, middle

# What about [TF 1 and TF 2](https://www.youtube.com/watch?v=t48a_KOh0fQ)?

---
class: middle

TensorFlow is constantly being modified and improved. But there was a major change when TF transitioned from TF1 to TF2.

---
class: middle

TF1 was originally developed and made available around 2015.

---
class: middle

But TF1 was somewhat hard to use and not intuitive for many users.

---
class: middle

Shortly after, libraries like Keras were developed to make the task of implementing DNNs easier.

---
class: middle

TF2 was released in 2019 and is closely integrated with Keras.

---
class: middle

The goal of TF2 was to:

- be higher-level than TF1 (e.g., less confusing code and simpler ways of writing common tasks and structures)
- simplified API
- greater versatility

---
class: middle

One of the big changes from TF1 to TF2 was that TF2 allows the so-called "eager" execution.

This is a technical point so we will not elaborate much.

---
class: middle

Briefly, in TF1, the user would have to first declare the structure of the DNN.

Then the DNN would be "compiled" and executed.

---
class: middle

In eager mode, we simply declare the sequence of operations to perform and the operations are evaluated immediately.

For us, this means that it's easier to get started and it eliminates a lot of boiler plate material.

---
class: center, middle

# What about PyTorch?

![:width 20vw](pytorch.png)

---
class: middle

We will not cover PyTorch in this class but this is also an excellent library.

It's very easy to use. 

Contrary to TF1, PyTorch had "eager" execution from the beginning.

---
class: middle

For now, it's unclear what the best tool is...

It's still a heated discussion.

---
class: middle

# Different ways of building DNNs in TF

- Sequential models
- Functional API
- Custom models

---
class: middle

The last part is the most general but also the most complicated. 

We will review it briefly for now.

---
class: middle

Sequential models are the easiest to work with.

They correspond to a sequence of layers where layers are simply "stacked."

Layer $i+1$ takes as input the output of layer $i$.

---
class: middle

The functional API allows writing more general models.

---
class: middle

Let's consider the following model:

- Input has size 1
- The model has 3 hidden layers of size 4. The activation function is tanh.
- The output has size 1 and uses a linear activation function.

---
class: middle

In Keras, this is very easy to do.

Declare a sequential model:

```Python
model = keras.models.Sequential()
```

---
class: middle

Construct all the layers:

```Python
model.add(keras.layers.InputLayer(input_shape=1))
model.add(keras.layers.Dense(4, activation="tanh"))
model.add(keras.layers.Dense(4, activation="tanh"))
model.add(keras.layers.Dense(4, activation="tanh"))
model.add(keras.layers.Dense(1, activation="linear"))
```

---
class: middle

`Dense` is what we have been using all along. That we apply a linear transformation of the form 

$$ W x + b$$

where $W$ is a dense matrix.

---
class: middle

`input_shape=1` is the size of the input. 

In our case, it's just $x$.

---
class: middle

We use the `tanh` activation function.

The last layer is simply a linear combination of hidden layer 3.

---
class: center, middle

![:width 80vw](2020-04-10-16-14-41.png)

---
class: middle

Various functions can be used to query the DNN.

---
class: middle

`model.summary()`

The output is:

---
class: middle

Output
 
    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 4)                 8         
    _________________________________________________________________
    dense_1 (Dense)              (None, 4)                 20        
    _________________________________________________________________
    dense_2 (Dense)              (None, 4)                 20        
    _________________________________________________________________
    dense_3 (Dense)              (None, 1)                 5         
    =================================================================
    Total params: 53
    Trainable params: 53
    Non-trainable params: 0

---
class: middle

The model is optimize using:

```Python
sgd = optimizers.SGD(lr=learning_rate)
model.compile(loss='mse', optimizer=sgd, metrics=['mse','mae'])   
```

---
class: middle

`SGD` is the stochastic gradient descent method.

We will explore later on how it works.

`lr` is the parameter that determines the step size in the gradient descent.

This is basically the parameter $\alpha$ we previously had.

---
class: middle

`loss` is the function that the optimize will try to minimize.

Common options are `mean_absolute_error`, `mean_squared_error`.

We will discuss other types of losses later on.

[Documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/losses)

---
class: middle

`metrics` are used to monitor convergence during training.

[Documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)

---
class: middle

To optimize or fit the model using the training data:

```Python
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid)) 
```                    

---
class: middle

An epoch is DNN corresponds to 1 iteration of the gradient descent method.

The parameter `epochs` controls how many iterations we should perform.

---
class: middle

The function

```Python
model.evaluate(X_test, y_test)
```

can be used to calculate the score or error for a given test set.

---
class: middle

```Python
model.predict(X_test)
```

can be used to predict the output using our DNN model.

---
class: middle

Please see the [Keras guide](https://www.tensorflow.org/guide/keras/overview) for additional examples.

---
class: center, middle

Let's consider an example. Let's say we want to approximate this function.

<iframe src="fig1.html" width="700" height="450" frameborder="0"></iframe>

---
class: center, middle

Here is the error convergence. We chose the mean squared error.

<iframe src="fig2.html" width="800" height="450" frameborder="0"></iframe>

---
class: center, middle

Model and true function

<iframe src="fig3.html" width="800" height="450" frameborder="0"></iframe>

---
class: center, middle

Absolute error

<iframe src="fig4.html" width="800" height="450" frameborder="0"></iframe>

---
class: middle

Let us now assume that we now want to code up the following DNN.

---
class: center, middle

![:width 80vw](2020-04-10-16-17-33.png)

---
class: middle

Hidden layers 1, 2, and 3 are fully connected with the previous layer.

But then, the output layer is fully connected to hidden layers 1, 2, and 3.

This cannot be expressed using a sequential DNN.

---
class: middle

We need to use the functional API for that.

It will look very similar to the previous syntax.

---
class: middle

```Python
from tensorflow.keras import layers, Model
input_ = layers.Input(shape=1)
hidden1 = layers.Dense(4, activation="tanh")(input_)
hidden2 = layers.Dense(4, activation="tanh")(hidden1)
hidden3 = layers.Dense(4, activation="tanh")(hidden2)
concat = layers.Concatenate()([hidden1, hidden2, hidden3])
output = layers.Dense(1, activation="linear")(concat)
model = Model(inputs=[input_], outputs=[output])
```

---
class: middle



    </textarea>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$']]
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script type="text/javascript">
        remark.macros.width = function (percentage) {
            var url = this;
            return '<img src="' + url + '" style="width: ' + percentage + '" />';
        };
        remark.macros.height = function (percentage) {
            var url = this;
            return '<img src="' + url + '" style="height: ' + percentage + '" />';
        };

        var slideshow = remark.create({
            ratio: '16:9',
            highlightLanguage: 'c++',
            highlightStyle: 'atom-one-light'
        });
    </script>
</body>

</html>