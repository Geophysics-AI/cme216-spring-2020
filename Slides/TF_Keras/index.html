<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <link rel="stylesheet" href="../cme216_slides.css">
</head>

<body>
    <textarea id="source">
class: center, middle

# CME 216, ME 343 - Spring 2020

## Eric Darve, ICME

![:width 40%](../Stanford.jpg)

---
class: center, middle

# What is TensorFlow?

![:width 20vw](tf.svg)

---
class: middle

TF can do many things. 

It's free and open source.

Obviously it can be used for deep learning.

---
class: middle

But at its base, it allows to express computations as a graph.

Then this graph can be differentiated, e.g., we can calculate gradients and solve optimization problems using gradient descent methods.

---
class: middle

For now, we will skip these general features of TF and focus on DNN but we will come back to that when discussing physics-informed machine learning or PhysML.

---
class: middle

TensorFlow was developed by the [Google Brain](https://research.google/teams/brain/) team for internal use at Google. 

Then it was made open source in November 2015.

---
class: center, middle

# What is Keras then?

![:width 20vw](keras.png)

---
class: middle

Keras is an API (application programming interface) for DNN calculations.

API means that Keras defines a specific interface to write computer programs.

Keras is written in Python.

---
class: middle

Keras has multiple backends (that is libraries that are responsible for doing the actual calculations).

---
class: middle

At this time they include:

- TensorFlow
- [CNTK](https://github.com/Microsoft/cntk) (from Microsoft)
- [Theano](https://github.com/Theano/Theano) (University of Montr&eacute;al, Canada)

---
class: middle

Compared to TF, Keras is focused on easy and fast prototyping, through

- user friendliness, 
- modularity, and 
- extensibility

---
class: middle

Although TF can be used as a backend for Keras, it is recommended to use `tf.keras`, which is the implementation of Keras in TF.

This is what we will do in this class.

---
class: center, middle

# What about [TF 1 and TF 2](https://www.youtube.com/watch?v=t48a_KOh0fQ)?

---
class: middle

TensorFlow is constantly being modified and improved. But there was a major change when TF transitioned from TF1 to TF2.

---
class: middle

TF1 was originally developed and made available around 2015.

---
class: middle

But TF1 was somewhat hard to use and not intuitive for many users.

---
class: middle

Shortly after, libraries like Keras were developed to make the task of implementing DNNs easier.

---
class: middle

TF2 was released in 2019 and is closely integrated with Keras.

---
class: middle

The goal of TF2 was to:

- be higher-level than TF1 (e.g., less confusing code and simpler ways of writing common tasks and structures)
- simplified API
- greater versatility

---
class: middle

One of the big changes from TF1 to TF2 was that TF2 allows the so-called "eager" execution.

This is a technical point so we will not elaborate much.

---
class: middle

Briefly, in TF1, the user would have to first declare the structure of the DNN.

Then the DNN would be "compiled" and executed.

---
class: middle

In eager mode, we simply declare the sequence of operations to perform and the operations are evaluated immediately.

For us, this means that it's easier to get started and it eliminates a lot of boiler plate material.

---
class: center, middle

# What about PyTorch?

![:width 20vw](pytorch.png)

---
class: middle

We will not cover PyTorch in this class but this is also an excellent library.

It's very easy to use. 

Contrary to TF1, PyTorch had "eager" execution from the beginning.

---
class: middle

For now, it's unclear what the best tool is...

It's still a heated discussion.

---
class: middle

# Different ways of building DNNs in TF

- Sequential models
- Functional API
- Custom models

---
class: middle

The last part is the most general but also the most complicated. 

We will review it briefly for now.

---
class: middle

Sequential models are the easiest to work with.

They correspond to a sequence of layers where layers are simply "stacked."

Layer $i+1$ takes as input the output of layer $i$.

---
class: middle

The functional API allows writing more general models.

---
class: middle

Let's consider the following model:

- Input has size 1
- The model has 3 hidden layers of size 4. The activation function is tanh.
- The output has size 1 and uses a linear activation function.

---
class: middle

In Keras, this is very easy to do.

Declare a sequential model:

```Python
model = keras.models.Sequential()
```

---
class: middle

Construct all the layers:

```Python
model.add(keras.layers.InputLayer(input_shape=1))
model.add(keras.layers.Dense(4, activation="tanh"))
model.add(keras.layers.Dense(4, activation="tanh"))
model.add(keras.layers.Dense(4, activation="tanh"))
model.add(keras.layers.Dense(1, activation="linear"))
```

---
class: middle

`Dense` is what we have been using all along. That we apply a linear transformation of the form 

$$ W x + b$$

where $W$ is a dense matrix.

---
class: middle

`input_shape=1` is the size of the input. 

In our case, it's just $x$.

---
class: middle

We use the `tanh` activation function.

The last layer is simply a linear combination of hidden layer 3.

---
class: center, middle

![:width 80vw](2020-04-10-16-14-41.png)

---
class: middle

Various functions can be used to query the DNN.

---
class: middle

`model.summary()`

The output is:

---
class: middle

Output
 
    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    dense (Dense)                (None, 4)                 8         
    _________________________________________________________________
    dense_1 (Dense)              (None, 4)                 20        
    _________________________________________________________________
    dense_2 (Dense)              (None, 4)                 20        
    _________________________________________________________________
    dense_3 (Dense)              (None, 1)                 5         
    =================================================================
    Total params: 53
    Trainable params: 53
    Non-trainable params: 0

---
class: middle

The model is optimize using:

```Python
sgd = optimizers.SGD(lr=learning_rate)
model.compile(loss='mse', optimizer=sgd, metrics=['mse','mae'])   
```

---
class: middle

`SGD` is the stochastic gradient descent method.

We will explore later on how it works.

`lr` is the parameter that determines the step size in the gradient descent.

This is basically the parameter $\alpha$ we previously had.

---
class: middle

`loss` is the function that the optimize will try to minimize.

Common options are `mean_absolute_error`, `mean_squared_error`.

We will discuss other types of losses later on.

[Documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/losses)

---
class: middle

`metrics` are used to monitor convergence during training.

[Documentation page](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)

---
class: middle

To optimize or fit the model using the training data:

```Python
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid)) 
```                    

---
class: middle

An epoch is DNN corresponds to 1 iteration of the gradient descent method.

The parameter `epochs` controls how many iterations we should perform.

---
class: middle

The function

```Python
model.evaluate(X_test, y_test)
```

can be used to calculate the score or error for a given test set.

---
class: middle

```Python
model.predict(X_test)
```

can be used to predict the output using our DNN model.

---
class: middle

Please see the [Keras guide](https://www.tensorflow.org/guide/keras/overview) for additional examples.

---
class: center, middle

Let's consider an example. Let's say we want to approximate this function.

<iframe src="fig1.html" width="700" height="450" frameborder="0"></iframe>

---
class: center, middle

Here is the error convergence. We chose the mean squared error.

<iframe src="fig2.html" width="800" height="450" frameborder="0"></iframe>

---
class: center, middle

Model and true function

<iframe src="fig3.html" width="800" height="450" frameborder="0"></iframe>

---
class: center, middle

Absolute error

<iframe src="fig4.html" width="800" height="450" frameborder="0"></iframe>

---
class: middle

Let us now assume that we now want to code up the following DNN.

---
class: center, middle

![:width 80vw](2020-04-10-16-17-33.png)

---
class: middle

Hidden layers 1, 2, and 3 are fully connected with the previous layer.

But then, the output layer is fully connected to hidden layers 1, 2, and 3.

This cannot be expressed using a sequential DNN.

---
class: middle

We need to use the functional API for that.

It will look very similar to the previous syntax.

---
class: middle

```Python
from tensorflow.keras import layers, Model
input_ = layers.Input(shape=1)
hidden1 = layers.Dense(4, activation="tanh")(input_)
hidden2 = layers.Dense(4, activation="tanh")(hidden1)
hidden3 = layers.Dense(4, activation="tanh")(hidden2)
concat = layers.Concatenate()([hidden1, hidden2, hidden3])
output = layers.Dense(1, activation="linear")(concat)
model = Model(inputs=[input_], outputs=[output])
```

---
class: middle

We recognize a new command

`layers.Concatenate()([hidden1, hidden2, hidden3])`

which concatenates together the output of multiple layers.

---
class: center, middle

Convergence

<iframe src="fig5.html" width="700" height="500" frameborder="0"></iframe>

---
class: center, middle

Error

<iframe src="fig6.html" width="700" height="500" frameborder="0"></iframe>

---
class: middle

Finally, we cover custom layers and models. This is done using Python subclassing (more on this later).

This is the most general technique to build DNNs.

---
class: middle

The Sequential API and the Functional API are declarative.

A declarative programming style is one where the user expresses the _logic_ of a computation without describing its _control flow._

Said otherwise, the user describes what the object should do but not directly how.

---
class: middle

The subclassing method is a type of imperative programming. 

That is, this is an approach where the user describes _how_ the program operates.

---
class: middle

# Imperative example.

You enter a restaurant and you say:

"I see that this table in the corner is empty. My wife and I are going to take it."

---
class: middle

# Declarative example.

You enter a restaurant and you say:

"A table for two, please."

---
class: middle

Subclassing requires to use Python inheritance.

You do not really need to know the details of this. 

If you know how to use the proper syntax, it is probably good enough.

But let us do a little more and explain what subclassing is and how it works in Python.

---
class: middle

Inheritance is a mechanism where new classes are derived (or built on) previous classes.

The class from which a class inherits is called the parent class or **superclass.**

A class that inherits from a superclass is called a **subclass,** also called heir class or child class.

---
class: middle

In Keras, you can subclass `tf.layers.Layer` and `tf.keras.Model`.

For simplicity, we will just look at subclassing `tf.keras.Model`.

---
class: middle

Here is the basic syntax:

```Python
class MyModel(keras.Model):
    def __init__(self, **kwargs):
        super().__init__(**kwargs) # handles standard args (e.g., name)
        [...]

    def call(self, input_):
       [...]
```

---
class: middle

In `__init__()` we will set up all the data-structures (layers) that are needed by our model.

`call()` defines the sequence of computations the DNN should perform.

---
class: middle

Python uses the method `__init__` to initialize the state of a new object of that class.

This is a constructor.

It is called when a new object of that class is created.

---
class: middle

The class `MyModel` derives from the class `keras.Model`.

Derived classes in Python inherit all the methods and properties from their parent classes.

In our case, `MyModel` inherits from `keras.Model`.

---
class: middle

Because we are subclassing, all the methods from `keras.Model` are available.

In particular, we can call the methods `compile`, `fit`, `predict`, and `evaluate` from `keras.Model`.

---
class: middle

`super().__init__(**kwargs)` allows calling the `__init__` method of the parent class.

This ensures that the constructor of the parent class (and potentially all the relevant ancestor classes) is called.

---
class: middle

`super()` is somewhat complicated to fully explain in this lecture.

To simplify the discuss, we will say that `super()` is referring to the parent class.

---
class: middle

See this demo Python code for details and examples using `super()`.

---
class: middle

For more information about `super()` see
the [super() Python doc](https://docs.python.org/3/library/functions.html#super)
and
this [blog](https://rhettinger.wordpress.com/2011/05/26/super-considered-super/) by Hettinger.

---
class: middle

`super()` is closely connected to the concept of the [method resolution order](https://docs.python.org/3/glossary.html#term-method-resolution-order).

See the `__mro__` attribute.

`super()` is great to call a function defined by a parent class. 

But it is most useful in cases of multiple inheritance.

---
class: middle

`__init__()`

```Python
def __init__(self, **kwargs):
    super().__init__(**kwargs) # handles standard args (e.g., name)
    self.hidden1 = keras.layers.Dense(4, activation="relu")
    self.hidden2 = keras.layers.Dense(4, activation="relu")
    self.hidden3 = keras.layers.Dense(4, activation="relu")        
    self.out = keras.layers.Dense(1, activation="linear")
```

---
class: middle

As we explained above, we first call `super().__init__()` so that the parent classes are initialized.

Then we build the three hidden layers and the output `self.out`.

---
class: middle

Note how, although we are defining 3 hidden layers and 1 output layer, we are not specifying how they are going to be used.

This will be done in `call()`.

---
class: middle

Compared to the previous case, we changed the activation function to `relu`.

---
class: middle

`call()`

```Python
def call(self, input_):
    hidden1 = self.hidden1(input_)
    hidden2 = self.hidden2(hidden1)
    hidden3 = self.hidden3(hidden2)        
    concat = layers.Concatenate()([input_, hidden3])
    return self.out(concat)
```

---
class: middle

`call` then defines the actual sequence of calculation to perform.

`self.hidden1(input_)` uses

`keras.layers.Dense(4, activation="relu")`

to calculate numerical values that are stored in `hidden1`.

---
class: middle

`concat` does not need to be part of the class since it is computed from `input_` and `hidden3`.

---
class: middle

We note that since we only define the layers and the sequence of operations, we have left a few things undefined.

For example, the size of `input_` is not defined yet.

The shape of the input is defined later when calling `fit`.

---
class: middle

The rest of the code is the same as what we had before.

```Python
model.compile(loss='mse',
              optimizer=sgd,
              metrics=['mse','mae'])
history = model.fit(X_train, y_train, epochs=n_epochs,
                    validation_data=(X_valid, y_valid))
```

---
class: center, middle

<iframe src="fig8.html" width="800" height="600" frameborder="0"></iframe>

---
class: middle

The `relu` activation is doing a little worse in this case.

---
class: middle

Finally, we show a different example where we use a different input.

The first observation is that the function is even.

So we could use as input $x^2$.

---
class: middle

But we can use more inputs as well.

Let's try

$$(2 x^2 - 1, 8 x^4 - 8 x^2 + 1)$$

These are even Chebyshev polynomials of order 2 and 4.

---
class: middle

Let us compare all these models.

---
class: center, middle

<iframe src="fig11.html" width="1000" height="600" frameborder="0"></iframe>

---
class: middle

`relu` has the worst performance.

`multi X` is relatively more efficient as it exhibits an error similar to `seq DNN` but uses half of the training data.

---
class: middle

Note how difficult it is to train these models to get high accuracy.

The convergence is rather slow.

    </textarea>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$']]
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script type="text/javascript">
        remark.macros.width = function (percentage) {
            var url = this;
            return '<img src="' + url + '" style="width: ' + percentage + '" />';
        };
        remark.macros.height = function (percentage) {
            var url = this;
            return '<img src="' + url + '" style="height: ' + percentage + '" />';
        };

        var slideshow = remark.create({
            ratio: '16:9',
            highlightLanguage: 'c++',
            highlightStyle: 'atom-one-light'
        });
    </script>
</body>

</html>