<!DOCTYPE html>
<html>

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
    <link rel="stylesheet" href="../cme216_slides.css">
</head>

<body>
    <textarea id="source">
class: center, middle

# CME 216, ME 343 - Winter 2020

## Eric Darve, ICME

![:width 40%](../Stanford.jpg)

---
class: middle

# scikit-learn

To demonstrate how SVM works we are going to use [scikit-learn](https://scikit-learn.org/). 

The results in this section can be reproduced using this shared
[notebook](https://colab.research.google.com/drive/1dSo81DdqIkzVyssauB7wYi12kZ0XzuVy). 

---
class: middle

The scikit-learn library can perform many important computations in machine learning including supervised and unsupervised learning.

See [scikit supervised learning](https://scikit-learn.org/stable/supervised_learning.html) for more details about the functionalities that are supported.

---
class: middle

We are going to demonstrate our concept through a simple example. 

Let's generate 8 random points in the 2D plane.

Points in the top left are assigned the label $-1$ ($y > x$) and points in the bottom right are assigned a label $-1$ ($y < x$).

---
class: middle

In Python, we set up two arrays X (coordinates) and y (labels) with the data: 

```python 
print('Shape of X: ', X.shape)
print(X)
print(' Shape of y: ', y.shape)
print(y)
```

---
class: middle

Output:
```
Shape of X:  (8, 2)
[[-0.1280102  -0.94814754]
 [ 0.09932496 -0.12935521]
 [-0.1592644  -0.33933036]
 [-0.59070273  0.23854193]
 [-0.40069065 -0.46634545]
 [ 0.24226767  0.05828419]
 [-0.73084011  0.02715624]
 [-0.63112027  0.5706703 ]]
Shape of y:  (8,)
[ 1.  1.  1. -1.  1.  1. -1. -1.]
```

---

Computing the separating hyperplane is done using
```python
from sklearn import svm
clf = svm.SVC(kernel="linear",C=1e6)
clf.fit(X, y)
```

`clf` now contains all the information of the SVM. We will learn later on what the constant `C` is.

---

To visualize the result, we can plot the black solid line that separates the points. 

Recall that the equation of the line is

$$ w^T x + b = 0 $$

---

$w$ is given by `clf.coef_`:

```python
print(clf.coef_)
[[ 2.1387469  -2.62113502]]
```

$b$ is given by `clf.intercept_`:

```python
print(clf.intercept_)
[0.63450173]
```

---

We can plot the points and line using [Plotly](https://plotly.com/python/) syntax

```python
x = np.linspace(-1, 1, 2)
a = -clf.coef_[0,0] / clf.coef_[0,1]
b = -clf.intercept_ / clf.coef_[0,1]
fig.add_trace(go.Scatter(x=x, y=a*x + b))
```

---

We can also visualize the support vectors.

There are 3 points in this case. 

```python
print(clf.support_vectors_)
[[-0.73084011  0.02715624]
 [-0.40069065 -0.46634545]
 [ 0.24226767  0.05828419]]
```

---

The lines going through these points satisfy the equations

Top line

$$ w^T x + b = -1 $$

Bottom line

$$ w^T x + b = 1 $$

---

These lines can be plotted using

```python
# green line
b1 = -(1 + clf.intercept_) / clf.coef_[0,1]
fig.add_trace(go.Scatter(x=x, y=a*x + b))
# purple line
b2 = -(-1 + clf.intercept_) / clf.coef_[0,1]
fig.add_trace(go.Scatter(x=x, y=a*x + b2))
```

---
class: center, middle

<iframe src="svm1.html" width="800" height="600" frameborder="0"></iframe>

---

The orange line is the "farthest" away from the red and blue dots. 

All the support vectors are at the same distance from the orange line.

---

The decision function, equal to $w^T x + b$ in our notations, can be computed using

```python
clf.decision_function(X)
```

where `X` contains the coordinates of the points where the function is to be evaluated. 

---
class: center, middle

<iframe src="svm1.html" width="800" height="600" frameborder="0"></iframe>

---

# Soft-margin

For many problems, though, because of noise and complex data, it is not possible to have a hyperplane that exactly separates the data. 

In that case, there is no solution to the optimization problem above.

---
class: center, middle

<iframe src="svm3.html" width="800" height="600" frameborder="0"></iframe>

---

The figure shows an example where no line divides the red dots from the blue dots. 

The optimization problem from the previous section has no solution in that case.

---

One solution is to introduce slack variables so that some constraints can be violated but in a minimal way

$$ y_i (w^T x_i + b) \ge 1 - \xi_i $$

with $\xi_i \ge 0$. 

---
class: middle

$$ y_i (w^T x_i + b) \ge 1 - \xi_i $$

If the constraint $y_i (w^T x_i + b) \ge 1$ can be satisfied, then $\xi_i = 0$.

---
class: middle

$$ y_i (w^T x_i + b) \ge 1 - \xi_i $$

If $1 > \xi_i > 0$, then the constraint is weakly violated but the classification is still correct. 

The sign of $w^T x_i + b$ (which is used to predict the label) is still the same as $y_i$. 

---
class: middle

$$ y_i (w^T x_i + b) \ge 1 - \xi_i $$

But, if $\xi_i > 1$ then the data is **misclassified.** 

The sign of $w^T x_i + b$ is now different from the sign of $y_i$. 

Hopefully this only happens for very few points.

---
class: middle

The new optimization problem becomes:
 
$$ (w,b,\xi) = \text{argmin}\_{w,b,\xi} \frac{1}{2} \|w\|\_2^2 + C \sum_{i=1}^n \xi\_i $$

with the following constraints:

$$ y_i (w^T x_i + b) \ge 1 - \xi_i $$

$$ \xi_i \ge 0 $$

---
class: middle

$C$ is set by the user and determines how much slack we are allowing. 

---
class: middle

A large $C$ means little violation is tolerated. 

Very few points are allowed to violate the condition. 

The hyperplane is strongly determined by the points nearest to the hyperplane. 

The hyperplane is very sensitive to points that violate the condition.

---
class: middle

A small $C$ means a lot of violations are possible. 

Small $C$ is required when data has a lot of noise that needs to be filtered out. 

In that case, many violations will be accepted as long as this leads to a large separation $1/\|w\|_2$.

---
class: center, middle

<iframe src="svm4.html" width="800" height="600" frameborder="0"></iframe>

---
class: middle   

Now we see that points are allowed to lie between the orange and green lines. 

There are even a few red points below the orange line and a few blue points above. 

But this cannot be avoided since no line perfectly separates the red points from the blue points.

---
class: middle

# Overfitting and underfitting

The value of `C` can be optimized in different ways. 

This is a broad topic and we will only cover the main ideas.

---
class: middle

`C` must be tuned based on how we trust the data. 

Generally speaking, if the data is very accurate (and a separating hyperplane exists) then `C` must be chosen very large. 

But if the data is noisy (we do not trust it) then `C` must be small.

---
class: middle

Let' s start by illustrating the effect of varying `C` in our method. 

We consider the following problem.

---
class: center, middle

<iframe src="svm5.html" width="800" height="600" frameborder="0"></iframe>

---
class: middle

We created two well-separated clusters with labels $-1$ and $+1$. 

Then we added a blue point on the left and a red point on the right. 

---
class: middle

The real line of separation is $y=x$ as before. 

So the outlier points can be considered as incorrect data here. 

Either this data was entered incorrectly in our database, or there was some large error in the measurements. 

---
class: middle

Let's pick a large value of `C` 

```python 
# fit the model 
clf=svm.SVC(kernel="linear" , C=10^4) 
clf.fit(X, y) 
``` 

---
class: middle

The SVM decision line has a negative slope as shown below.

---
class: center, middle

<iframe src="svm6.html" width="800" height="600" frameborder="0"></iframe>

---
class: middle

The red point on the right is classified with a label $-1$ (red-orange
region). 

And similarly for the blue point. 

---
class: middle

However, we know that these points are erroneous, and therefore the
classification is wrong here. 

This is a problem of _overfitting_. 

We trust too much the data which leads to a
large error. 

---
class: middle

We can try again using a small `C`. 

However, now the model believes that there is a large error in all the data. 

As a result, the prediction is quite bad. 

---
class: middle

```python 
clf=svm.SVC(kernel="linear" , C=0.2)
clf.fit(X, y)``` 

---
class: center, middle

<iframe src="svm7.html" width="800" height="600" frameborder="0"></iframe>

---
class: middle

This case corresponds to a situation of _underfitting_. 

That is we apply too much regularization by reducing `C` and do not trust enough the data. 

If we pick `C=0.3`, we get a better fit in this case. 

---
class: middle

```python 
clf=svm.SVC(kernel="linear" , C=0.3) 
clf.fit(X, y) 
``` 

---
class: center, middle

<iframe src="svm8.html" width="800" height="600" frameborder="0"></iframe>

---
class: middle

This plot is intermediate between the previous plots. 

We trust the outlier points but only to a moderate extent. 

---
class: middle

The solid red line is the line $y=x$ but because of the outlier points, it is not possible in this case to recover that answer. 

The SVC model is always biased by the outliers to some extent. 

---

# Training and validation sets

This leads us to the general question of how we should pick $C$. 

This is a problem that we will explore again in the future for other methods. 

---
class: middle

In machine learning, errors may arise from: 

- Error in the data; noise in the data may prevent us from finding an exact answer. 

- The model may be overly sensitive to small changes in the data (e.g., large conditioning) and needs to be "stabilized." 

---
class: middle

Because of this, it is common to use a regularization strategy. 

In our case, this was done by varying $C$.

---
class: middle

A small $C$ corresponds to a lot of regularization. 

This leads to a small vector $w$ and if $C$ is too small, the vector $w$ and scalar $b$ may become significantly wrong. 

---
class: middle

A large $C$ corresponds to minimal regularization. 

In that case, we assume the data is accurate and look for a hyperplane that optimally separates
the data (e.g., the hyperplane maximizes the distance to all points). 

---
class: middle

A typical strategy consists of the following. 

We define two sets of points, called the _training_ set and the _validation_ set. 

- Training set: this set is used to fit the model. In our case, this is used to calculate $(w,b,\xi)$. 
- Validation set: the set is used to tune some of the model parameters, in our case $C$. It is usually used to control over- or
under-fitting. 

---
class: middle

The optimization may be based on an iterative loop where we perform in turn 

- $(w,b,\xi)$: optimized based on training set 
- parameter $C$: optimized based on validation set

---
class: middle

Let's give an example to illustrate how this may work in practice. 

We consider our previous test case where the input data is randomly perturbed. 

```python 
# randomly perturb the points X 
for i in range(0,X.shape[0]): 
    X[i,:] = X[i,:] + (2*np.random.rand(1, 2) - 1 ) / 2 
``` 

---
class: center, middle

<iframe src="svm3.html" width="800" height="600" frameborder="0"></iframe>

---
class: center, middle

scikitlearn provides a few functionalities that can be used to simplify the process. 

---
class: middle

Let's start by splitting the input data into a training and validation set.

```python 
from sklearn.model_selection import train_test_split 
X_train, X_valid, y_train, y_valid=train_test_split(X, y, test_size=0.4)
``` 

`test_size` is the fraction of the input data that will be used for the validation set. 

---
class: middle

The word `test` is used because this function is usually used to split the data into a training set and a test set.

But, for our application, we will use the same function to split the data into a training set and a validation set to control over-fitting. 

---
class: middle

We can verify that the sets have the correct sizes (60% and 40%): 

```python 
print('Size of X_train: ',X_train.shape[0])
print(' Size of X_test: ',X_valid.shape[0])
Size of X_train:  19
Size of X_valid:  13
```

---
class: middle

The `clf.score()` function can be used to evaluate the accuracy of our SVM prediction using the test set. 

The goal is then to find the value of $C$ that gives us the highest score.

We obtain the following results.

---
class: middle

<iframe src="svm9.html" width="1000" height="800" frameborder="0"></iframe>

---
class: middle

$a$ is the slope of the predicted decision line. 

The exact solution is $y=x$ and so $a=1$ is the exact solution. 

$b$ is the predicted intercept. The exact solution is $b=0$.

---
class: middle

We see that values of $C$ below 1 lead to under-fitting (too much regularization). 

Larger values, with $C>5$, give consistently good results.

---
class: middle

# Cross-validation

We have seen a simple way to decompose the set into a training set and validation set. 

However, this may lead to some issues. 

In particular, if we make a unique choice of training and validation set it is quite possible that our prediction becomes biased by our choice of validation set. 

---
class: middle

Indeed, only the validation set is used to estimate how good $C$ is and whether it should be adjusted. 

So to overcome this, it is preferable to optimize our model for many training sets and evaluate its performance on many validation sets. 

But how can this be done? This would require a tremendous amount of data.

---
class: middle

In practice, the method of cross-validation is used. 

The dataset at our disposal is split into $K$ groups. 

Then we run a series of experiments in which we use $K-1$ groups of data for optimizing the model, and the remaining group for validation (i.e., to score our model). 

---
class: middle

By "reusing" our dataset in this fashion, we are able to generate many scores with limited data. 

These scores can then be averaged to estimate how good $C$ is. 

Based on this result $C$ can be optimized. 

---
class: middle

The advantage of this approach is that we minimize a possible bias due to our selection of training and validation sets, without requiring a tremendous amount of data.

---
class: middle

For more information on this, please see [scikit-learn cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html). 

There are many variants on this idea.

---
class: middle

The [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html) section in scikit-learn and the [wikipedia page](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets) on training and validation sets introduce as well the concept of _test set._ 

---
class: middle

Here is a summary of what each one of these sets is used for.

---
class: middle

- Training set: this set is used to opimize the model, in our case $w$ and $b$.
- Validation set (also called development set) is used to optimize hyper-parameters in the method, for example the parameter $C$ in our example. This can be used to control overfitting and for other optimization of parameters that is not part of step 1 with the training set.

---
class: middle

- Once the model has been computed and that all parameters have been optimized based on the available data, we use a yet-unseen dataset, the test set, to evaluate the accuracy of our model.

By definition, the test set is applied to the final model. 

No further changes are made to the model during that stage. 

---
class: center, middle

See [scikit-learn cross_validation](https://scikit-learn.org/stable/modules/cross_validation.html) and [training, validation, and test sets](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets).

---
class: middle

# Kernel trick

An important extension of this method allows treating decision surfaces (the hyperplane that divides points with different colors) into more general surfaces.

---
class: middle

To understand this connection, we start from the form of our prediction model

$$ w^T x + b $$

Given some training points $x^{(i)}$ we can find coefficients $\alpha_i$ such that

$$ w^T x + b = \sum_i \alpha_i [x^{(i)}]^T x + b $$

---
class: middle

So far this is just a change in notation. 

However, it reveals that we can view $w^T x$ as dot products between $ x^{(i)} $ and $x$. 

An important extension is to realize that it is possible to use other coordinates than $x$. 

These are sometimes called features. 

---
class: middle

Imagine now that we have at our disposal a function $\phi(x)$ that is vector-valued. 

This represents a set of "features" representing the data $x$. 

---
class: middle

For example, instead of considering $$x = (x_1,x_2)$$ we could define

$$ \phi(x) = (x_1, x_2, x_1^2, x_2^2, x_1 x_2) $$

---
class: middle

Then we may use as our model to predict a label

$$ \sum_i \alpha_i \phi(x^{(i)})^T \phi(x) + b $$

When this function is positive, we predict the label $+1$ and $-1$ otherwise.

---
class: middle

However, in many cases, it may be difficult to find an appropriate $\phi(x)$, and if $\phi$ is very high-dimensional (a lot of features) it may be expensive to compute the dot product 

$$\phi(x^{(i)})^T \phi(x)$$

---
class: middle

The kernel trick is an ingenious idea. It replaces the expression above by

$$ \sum_i \alpha_i K(x^{(i)},x) + b $$

---
class: middle

There are several theoretical justifications and explanations for this approach but here we will simply demonstrate this approach through examples. 

Please read [Kernel methods in machine learning](https://projecteuclid.org/download/pdfview_1/euclid.aos/1211819561) by Hofman, Sch&ouml;lkopf and Smola for mathematical details on this method.

---
class: middle

Many different types of kernels can be used. For example in scikit-learn, we have

Kernel  | Definition
---     | ---
Linear  | $\langle x, x' \rangle$ 
Polynomial | $(\gamma \langle x, x' \rangle + r)^d$ 
Radial basis function (RBF) | $\exp(-\gamma \lVert x - x' \rVert^2)$ 
Sigmoid | $\tanh(\gamma \langle x, x' \rangle + r)$ 

---
class: middle

We now demonstrate this method on a simple example. 

We consider a situation where the blue points on top are separated from the red points on the bottom by a sine function. 

Since a linear SVM uses a line to separate these points it cannot make a good prediction. 

---
class: center, middle

<iframe src="svm10.html" width="800" height="600" frameborder="0"></iframe>

---
class: center, middle

When we use an RBF, the results are much more accurate.

---
class: center, middle

<iframe src="svm11.html" width="800" height="600" frameborder="0"></iframe>

---
class: center, middle

See Section 5.7.2 in [Deep Learning](https://www.deeplearningbook.org/) for more details on the kernel trick.

---
class: middle

This is done using 

```python 
# fit the model 
clf=svm.SVC(kernel="rbf" , gamma=10)
clf.fit(X, y) 
```

---
class: middle

As before, we can tune the regularization parameter `C` to improve the fit

The parameter `gamma` controls the width of the Gaussian function $\exp(-\gamma \lVert x - x' \rVert^2)$.
</textarea>
    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$']]
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>
    <script src="https://remarkjs.com/downloads/remark-latest.min.js">
    </script>
    <script type="text/javascript">
        remark.macros.width = function (percentage) {
            var url = this;
            return '<img src="' + url + '" style="width: ' + percentage + '" />';
        };
        remark.macros.height = function (percentage) {
            var url = this;
            return '<img src="' + url + '" style="height: ' + percentage + '" />';
        };

        var slideshow = remark.create({
            ratio: '16:9',
            highlightLanguage: 'c++',
            highlightStyle: 'atom-one-light'
        });
    </script>
</body>

</html>