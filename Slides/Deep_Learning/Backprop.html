---
layout: slides
---

class: center, middle

# CME 216, ME 343 - Spring 2020

## Eric Darve, ICME

![:width 40%](../Stanford.jpg)

---
class: middle

In this lecture we are going to look at methods to compute the gradient of the loss function.

---
class: middle

As we have seen, DL problems are formulated as a problem of minimizing a loss function $L$ that depends on the weights and biases of the DNN.

---
class: middle

To do that, we will use gradient-based algorithms. That is we update the weights and biases using

$$\Delta W^{(k)} = - \alpha \frac{\partial L}{\partial W^{(k)}}$$

$$\Delta b^{(k)} = - \alpha \frac{\partial L}{\partial b^{(k)}}$$

---
class: middle

$W^{(k)}$: weights of layer $k$.

$b^{(k)}$: biases of layer $k$.

---
class: middle

We will see more advanced optimization methods later, but they all require the computation of the gradient of the loss function with respect to the weights and biases.

Let's see how the gradient can be computed.

---
class: middle

An efficient algorithm has been designed for this.

It's called the back-propagation algorithm.

---
class: middle

As we have seen a DNN is a function of the type

$y(x) = $

$W^{(n)} \phi \odot W^{(n-1)} \cdots \phi \odot W^{(2)} \phi \odot W^{(1)} x$

---
class: middle

$\phi \; \odot$
 
This corresponds to applying the non-linearity $\phi$ element-wise to a vector.

---
class: middle

For example: $\phi \odot W^{(1)} x$

This corresponds to a multiplication by matrix $W^{(1)}$. 

Then we apply the non-linear function $\phi$ element-wise.

---
class: middle

In this formula we are seemingly not using any bias $b$.

But the following trick can be used:

$$w^T x + b = [w, b]^T \begin{pmatrix} x\\\\ 1 \end{pmatrix}$$

---
class: middle

The bias $b$ can be represented by adding an entry at the end of $w$ and appending a 1 at the end of $x$.

So by changing our definition of $x$, we can represent our DNN using weights $W$ only.

---
class: middle

This will simplify our notations in what will follow.

---
class: middle

Our DNN is the result of the following composition of functions:

$W^{(n)} \phi \odot W^{(n-1)} \cdots \phi \odot W^{(2)} \phi \odot W^{(1)} x$

---
class: middle

We need to apply the chain rule to calculate the gradient.

It gets pretty complicated.

---
class: middle

So grab a pencil and paper and please follow along the equations as we go.

---
class: middle

The trick is to apply the chain rule starting from the left of this expression, progressively moving to the right.

$$W^{(n)} \phi \odot W^{(n-1)} \cdots \phi \odot W^{(2)} \phi \odot W^{(1)} x$$

---
class: middle

You could do the same thing going from the right to the left but an analysis reveals that this is computationally more expensive.

$$W^{(n)} \phi \odot W^{(n-1)} \cdots \phi \odot W^{(2)} \phi \odot W^{(1)} x$$

---
class: middle

Let us define:

$$a^{(k)} = \phi \odot W^{(k)} \cdots \phi \odot W^{(1)} x$$

---
class: middle

So

$$y = W^{(n)} \phi \odot W^{(n-1)} \cdots \phi \odot W^{(k+1)} a^{(k)}$$

Recall:

$y(x) = $

$W^{(n)} \phi \odot W^{(n-1)} \cdots \phi \odot W^{(2)} \phi \odot W^{(1)} x$

---
class: middle

We will use a recurrence relation.

It will start from $k=n$ and will go down to $k=0$.

---
class: middle

Let's start with 

$$ \frac{\partial y}{\partial W^{(n)}} $$

We can always assume that $y$ is a scalar.

---
class: middle

We have

$$ y = W^{(n)} a^{(n-1)} $$

The trick is that $a^{(n-1)}$ is independent of $W^{(n)}$.

So $y$ is a linear function of $W^{(n)}$.

---
class: middle

Since $y$ is a scalar, the last matrix $W^{(n)}$ must be a row vector.

---
class: middle

Let's calculate a derivative with respect to a single component:

$$ \frac{\partial y}{\partial [W^{(n)}]_j} = [a^{(n-1)}]_j $$

---
class: middle

Using matrix notations, this becomes

$$ \frac{\partial y}{\partial [W^{(n)}]} = [a^{(n-1)}]^T $$

where $[a^{(n-1)}]^T$ is a row vector.

---
class: middle

Let's go on to $W^{(n-1)}$ to see how this works.

We now have:

$$y = W^{(n)} \phi \odot W^{(n-1)} a^{(n-2)}$$

---
class: middle

Let's differentiate with respect to the $(i,j)$ component of $W^{(n-1)}$.

Let's denote $E_{ij}$ a matrix full of zeros with a 1 at index $(i,j)$.

---
class: middle

Then

$$\frac{\partial y}{\partial [W^{(n-1)}]\_{ij}} = W^{(n)} \, \psi^{(n-1)} \, E_{ij} \, a^{(n-2)}$$

$\psi^{(n-1)}$ is a diagonal matrix with entries

$$[\psi^{(n-1)}]\_{ii} = [ \phi' \odot W^{(n-1)} a^{(n-2)} ]\_i$$

---
class: middle

The scalar $W^{(n)} \, \psi^{(n-1)} \, E_{ij} \, a^{(n-2)}$ can be re-written using vectors:

$$ \frac{\partial y}{\partial [W^{(n-1)}]\_{ij}} = [W^{(n)} \psi^{(n-1)}]_i \; [a^{(n-2)}]_j$$

$W^{(n)} \psi^{(n-1)}$ is a row vector

$a^{(n-2)}$ is a column vector

---
class: middle

Let's denote

$$\delta^{(n)} = \psi^{(n-1)} [W^{(n)}]^T$$

---
class: middle

Using matrix notations we get

$$\frac{\partial y}{\partial W^{(n-1)}} = \delta^{(n)} \, [a^{(n-2)}]^T$$

$\delta^{(n)}$ is a column vector

$[a^{(n-2)}]^T$ is a row vector.

$\frac{\partial y}{\partial W^{(n-1)}}$ is a matrix

---
class: middle

Let us briefly repeat this for $W^{(n-2)}$ to see how this works.

We now have:

$$y = W^{(n)} \phi \odot W^{(n-1)} \phi \odot W^{(n-2)} a^{(n-3)}$$

---
class: middle

 Differentiate:

$ \frac{\partial y}{\partial [W^{(n-2)}]\_{ij}} = $

$ = [ W^{(n)} \psi^{(n-1)} W^{(n-1)} \psi^{(n-2)} ]_i \, [a^{(n-3)}]_j $

---
class: middle

In matrix form:

$$ \frac{\partial y}{\partial [W^{(n-2)}]} = \delta^{(n-1)} \, [a^{(n-3)}]^T $$

$$\delta^{(n-1)} = \psi^{(n-2)} [W^{(n-1)}]^T \psi^{(n-1)} [W^{(n)}]^T$$

---
class: middle

We can now define the general formula.

There is a recurrence relation for $\delta^{(k)}$.

$$ \delta^{(k)} = \psi^{(k-1)} \, [W^{(k)}]^T \, \delta^{(k+1)}, \qquad \delta^{(n+1)} = 1 $$

$$[\psi^{(k-1)}]\_{ii} = [ \phi' \odot W^{(k-1)} a^{(k-2)} ]\_i$$

---
class: middle

Equation for gradient with respect to matrix $W^{(k)}$

$$ \frac{\partial y}{\partial [W^{(k)}]} = \delta^{(k+1)} \, [a^{(k-1)}]^T $$

---
class: middle

## Explicit expression for the bias

We previously said that the bias can be represented in this framework by adding a 1 at the end of the activation $a^{(k)}$.

With this trick, we can immediately find an explicit expression for the gradient with respect to the bias.

---
class: middle

## Gradient with respect to the bias

$$ \frac{\partial y}{\partial [b^{(k)}]} = \delta^{(k+1)} $$

---
class: middle

The implementation can be done in two passes called the forward and backward passes.

---
class: middle

In the following, for completeness we reintroduce the biases.

---
class: middle

## Forward pass

We compute all the activations: $k$ goes from 1 to $n$.

$$a^{(k)} = \phi \odot ( W^{(k)} a^{(k-1)} + b^{(k)} )$$

$$a^{(0)} = x$$

---
class: middle

We also need to save the values of the derivative:

$$[\psi^{(k)}]\_{ii} = [ \phi' \odot ( W^{(k)} a^{(k-1)} + b^{(k)} ) ]\_i$$

---
class: middle

## Backward pass

For $k$ going from $n$ to $1$:

$$ \delta^{(k)} = \psi^{(k-1)} \, [W^{(k)}]^T \, \delta^{(k+1)} $$

$$ \delta^{(n+1)} = 1 $$

---
class: middle

 Derivatives:

$$ \frac{\partial y}{\partial [W^{(k)}]} = \delta^{(k+1)} \, [a^{(k-1)}]^T $$

$$ \frac{\partial y}{\partial [b^{(k)}]} = \delta^{(k+1)} $$